{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the problem. What is the input? What is the output? What data mining function is required? What could be the challenges? What is the impact? What is an ideal solution?\n",
    "\n",
    "    - The problem: predict if a specific reddit post is fake news or not, by looking at its title. \n",
    "    - The input: news posts\n",
    "    - the output: if news post is fake or not based on its title.\n",
    "    - Hence the output is fake or not then the data mining function: is binary classification\n",
    "    - Challenges: Problems in spelling, a lot of stop words, unbalanced labels\n",
    "    - the impact: detecting if news is fake or not so we can prevent it before people can see it.\n",
    "    - the solution: is to do tokenazation and count words then give them weights based on their occurances then use some machine learning model like random forest or xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
    "\n",
    "    - An n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be characters, words, or other units of text. A character n-gram is a sequence of n characters from a given text. For example, the 2-character (or bigram) n-grams of the word “text” are “te”, “ex”, and “xt”.A word n-gram is a sequence of n words from a given text. For example, the 2-word (or bigram) n-grams of the sentence “I love to read” are “I love”, “love to”, and “to read”.\n",
    "    \n",
    "    - The OOV (out-of-vocabulary) issue refers to the problem of encountering words or tokens that were not seen during training. Word n-grams tend to suffer more from the OOV issue than character n-grams. This is because the vocabulary of word n-grams is typically much larger than the vocabulary of character n-grams, and it is more likely that a new word will be encountered that was not seen during training. In contrast, the vocabulary of character n-grams is limited to the set of possible characters in the language, which is typically much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "\n",
    "    - Stop word removal and stemming are two different techniques used in natural language processing. Stop word removal involves removing common words that do not carry much meaning, such as “the”, “is”, “and”, etc. Stemming involves reducing words to their root form, for example, “running” would be reduced to “run”. These techniques can be language-dependent as stop words and word forms vary between languages.\n",
    "    Yes, these techniques can be language-dependent as stop words and word forms vary between languages. For example, the list of stop words used for English text would be different from the list used for French text. Similarly, stemming algorithms are often designed for specific languages and may not work as well for other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Is tokenization techniques language dependent? Why?\n",
    "\n",
    "- Yes, tokenization techniques can be language-dependent. Tokenization is the process of breaking down a piece of text into small units called tokens. A token may be a word, part of a word or just characters like punctuation. Every language has its own grammatical constructs, which are often difficult to write down as rules. These issues of tokenization are language-specific and require the language of the document to be known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "\n",
    "- CountVectorizer and TfidfVectorizer are both methods used in natural language processing to vectorize text. CountVectorizer simply counts the number of times a word appears in a document (using a bag-of-words approach), while TfidfVectorizer takes into account not only how many times a word appears in a document but also how important that word is to the whole corpus. This is done by penalizing words that often appear across all documents, reducing the count of these as these words are likely to be less important. It would not be feasible to use all possible n-grams as the number of possible n-grams grows exponentially with the length of the n-grams. Instead, it is common to select a range of n-gram lengths (e.g., unigrams, bigrams, and trigrams) and only consider n-grams within that range.Would you like more information on this topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/24/14/d9ecb9fa86727f51bfb35f1c2b0428ebc6cd5ffde24c5e2dc583d3575a6f/xgboost-1.6.2-py3-none-win_amd64.whl (125.4MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from xgboost) (1.16.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.2\n"
     ]
    }
   ],
   "source": [
    "#! pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    " # importing important liabraries\n",
    "from IPython.display import display_html\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.stats import gaussian_kde\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain,cycle\n",
    "from collections  import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "import nltk\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dictonaries below are words that we gonna handdle and replace its abbrevaiton by the complete word\n",
    "contractions = {\"'re\":'are',\"u're\":\"you are\",\"they've\":\"they have\",\n",
    "                \"i'm\":\"i am\",\"you've\":\"you have\",\"we've\":\"we have\",\"wouldn't\":'would not',\n",
    "                \"couldn't\":'could not',\"isn't\":'is not',\"don't\":'do not',\"haven't\":'have not',\n",
    "                \"hasn't\":'has not',\"doesn't\":'does not',\"I've\":'i have',\"aren't\":'are not',\n",
    "                \"shouldn't\":'should not',\"can't\":'can not',\"hadn't\":'had not',\"wasn't\":'was not',\n",
    "                \"won't\":'will not',\"there's\":'there is',\"didn't\":'did not',\"weren't\":'were not',\n",
    "                \"that's\":'that is',\"he's\":'he is',\"she's\":'she is',\"they're\": 'they are',\n",
    "                \"cya\":'see you',\"we're\":'we are',\"it's\":'it is',\"you're\":\"you are\",\"i've\":'i have',\n",
    "                \"wtf\":'what the fuck',\"must've\":'must have',\"should've\":\"should have\",\"would've\":\"would have\",\n",
    "                \"could've\":\"could have\",\"how're\":'how are',\"it's\":'it is',\"what's\":'what is',\"\\\\\":\"\",\"\\\\n\":\"\",\"cuz\":\"because\", \"cant\": \"can not\"}\n",
    "numer_words = {'sherfield72':'mention','rs40000cr':'mention','air1bullet':'air bullet','960kzim':'mention',\n",
    "               \"'revel8ion\":'revelation',\"'news786\":'news 786',\"'mumbai24x7\":'mention','17months':'17 months',\n",
    "               'ny1burst':'new york burst','tkyonly1fmk':'mention','nikoniko12022':'mention','hamosh84':'mention',\n",
    "               'ibliz140':'mention','bkb066gp':'mention','tksgs0810':'mention','mhtw4fnet':'mention',\"'psalm34\":'mention',\n",
    "               \"76mins\":'76 minutes',\"oliviamiles01\":'mention','friend50':'friend 50', \"'master0fsloths\":'master of sloths',\n",
    "               \"'s3xleak\":'sex leak','push2left':'push to left',\"'411naija\":'mention','wsvr1686b':'mention','911bombing':'911 bombing',\n",
    "               \"fuckjf9jjzs',\":'mention','b4federal':'mention','7amdollela':'7 am dollela','3million': '3 million','vaping101':'mention',\n",
    "               \"'matako_3\":'mention','150bilno':'mention',\"'neil_eastwood77\":'mention','hirochii0':'mention',\"20mins',\":'20 mins',\n",
    "               \"'chickmt123\":'mention','drayesha4':'mention','death2usa':'death to usa',\"2gether',\":'together','time2015':'time 2015',\n",
    "               \"'nflweek1picks\":'mention','martinmj22':'mention',\"'lizzie363\":'mention','russaky89':'mention','friend59':'mention',\n",
    "               \"you5jbwgge',\":'mention'}\n",
    "\n",
    "word_numbers = {'06jst':'', '0npzp':'', '10x':'', '12jst':'', '12news':'', '12v':'', '18jst':'', '1x1':'', '24v':'', '325ci':'', \n",
    "                '3x':'', '3x5':'', '429cj':'', '43c':'', '4x4':'', '53inch':'', '6c':'', '712c':'', '_':'', '__':'', '_ah':'', \n",
    "                '_deo':'', '_keits':'', '_that':'', '_turns':'',\"',\":\"\",\"sherfield72\":\"mention\",\"pemantaujkt48\":\"mention\",\n",
    "                \"bracelet10mm\":\"mention\", \"'chickmt123\":\"mention\",\"'tarmineta3\":\"mention\",\"you5jbwgge',\":'mention',\n",
    "                \"'mumbai24x7\":'mention','5sosglobalsquad':'mention','dylanmcclure55':'mention','tanstaafl23':'',\n",
    "                'footballfreestyle24':'football free style 24',\"40hourfamine\":\"40 hour famine\",\"15000270364',\":'number',\n",
    "                'dylanmcclure55':'mention ',\"'master0fsloths\":'master of sloths','insurers163':'insurers 163',\"15000270653',\":'number',\n",
    "                'zonewolf123':'mention',  \"'tkyonly1fmk\":'mention', \"sherfield72'\":'mention ', \"8437150124',\":'number',\n",
    "                '40hourfamine':'40 hour famine','watchmanis216':'mention','tarmineta3':'mention',\"'neil_eastwood77\":'mention',\n",
    "                \"oliviamiles01',\":'mention','hyider_ghost2':'mention ','rabidmonkeys1':'mention ','since1970the':'since 1970 the',\n",
    "                \"'nflweek1picks\":'mention','then0mads0ul':'mention',\"fuckjf9jjzs',\":'','janenelson097':'mention','087809233445':'0',\n",
    "                'onlinemh370':'mention', '2slow2report':'to slow to report',\"'nikoniko12022\":'mention', 'shawie17shawie':'mention'}\n",
    "\n",
    "repetead_vowels = {\"soooo\":\"so\",\"baaaack\":\"back\",\"aaaaaaallll\":\"all\",\"sparxxx\":\"mention\",\n",
    "                   \"oooh\":\"oh\",\"lmfaooo\":\"laughing my fat ass off\",\"fuckkkkkk\":\"fuck\",\"vuuuuu\":\"vu\",\n",
    "                   \"mxaaaa\":\"mention\",\"caaaaaall\":\"call\",\"goooooooaaaaaal\":\"goal\",'shoook':'shook',\n",
    "                   'maaaaan':'man','sooooooo':'soon','blaaaaaaa':'bla','youuu':'you','yeeessss':'yes',\n",
    "                   'noooo':'no','totoo':'totoo',\"ooohshit\":'oh shit','nooo':'no','wompppp':'womp','maddddd':'mad',\n",
    "                   \"'thankkk\":\"thank\",\"plsss\":\"please\",'ohhhh':'oh',\"'xoxoxxxooo\":'hugs and kisses',\n",
    "                   \"'damnnnn\":\"damn\",\"'damnnnn\":\"damn\",\"ooo\":\"oo\",\"oooo\":\"oo\",\"ooooo\":\"oo\",\n",
    "                   \"ssssss\":\"ss\",\"ssss\":\"ss\",\"yyyy\":'y',\"yyyy\":'y',\"lll\":\" ll \",\"eee\":\"ee\",\n",
    "                   \"wwwww\":\"w\",\"aaa\":\"a\",\"nnn\":\"n\",\"rrr\":\"r\",\"aaaaaa\":\"a\",\"gggg\":\"g\",\n",
    "                   \"wwww\":\"w\",\"www\":\"w\",\"mmmm\":\"m\",\"iiii\":\"i\",\"iii\":\"i\",\"sss\":'s',\"yyy\":\"y\",\n",
    "                   \"uuu\":\"u\",\"eee\":'e','hhh':'h',\"ddd\":\"d\",\n",
    "                   \"zzzz',\":\"sleep\",\"xxx',\":\"\",\"mhmmm\":\"hmm\",\"hmmm\":'hmm', '\"': ''}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') # download stop words\n",
    "\n",
    "\n",
    "def cont_to_exp(x, list_): #Used to expand contractions such has I'm to I am \n",
    "    x = str(x).lower()\n",
    "    if type(x) is str:\n",
    "        for key in list_:\n",
    "            value = list_[key]\n",
    "            x = x.replace(key,value)\n",
    "        return x\n",
    "    else :\n",
    "        return x\n",
    "    \n",
    "# the function below replace links and mentions with `link` or 'mention' word    \n",
    "def Prepro_1(text): \n",
    "    Ssymbols = [\"\\x89\",\"\\x9d\"]    \n",
    "    ''' Lower Replace #@mentions with Mention to save that pattern'''\n",
    "    x = re.sub(r'(https?://[^\\s]+)',\" LINK \",text) #Link\n",
    "    x = re.sub(r'\\S*(\\w+)(_)(\\w+)\\S*',\"MENTION\",x) #Mention\n",
    "    x = re.sub(r'(?<![@\\w])@(\\w{1,25})',\" MENTION \",x) #Mention\n",
    "    x = re.sub(r\"#(\\w+)\\S\",\" MENTION \",x) #MENTION\n",
    "    x = re.sub(r\"[âãåçèêìïñòó÷ûüª©¨¤«£¢¬`\\...¡!¼#\\$%&\\()\\*\\+,-./:;<=>?@\\[\\]^_`{|}~,]+\",\" \", x)\n",
    "    x = re.sub(r\"'(?!s)\",\"\", x)\n",
    "    x = re.sub(r\"\\\\n|\\\\n\\\\\",\" \" ,x) # valdiate those spaces /n that python was not able to decode.\n",
    "    x = x.lower() # make it lower case.\n",
    "    x = re.sub(str(Ssymbols),\" \",x)\n",
    "    return x.lower()\n",
    "# the function below do lemmatization\n",
    "def lematization(corpus):\n",
    "    lemmatizer = nltk.WordNetLemmatizer() # create lemmatizer object\n",
    "    NEC = [word for word in corpus.split() if word.lower() not in stopwords.words('english')] # removing stop words\n",
    "    lematized_words = [] # list to save lemmatized words\n",
    "    for word in NEC:\n",
    "        verb = lemmatizer.lemmatize(word, 'v') # lemmatize verbs\n",
    "        adj = lemmatizer.lemmatize(word, 'a')  # lemmatize words\n",
    "        none = lemmatizer.lemmatize(word, 'n')  # lemmatize nones\n",
    "        adv = lemmatizer.lemmatize(word, 'r')  # lemmatize nones\n",
    "        if verb != word: # if the verb variable changed \n",
    "            lematized_words.append(verb)\n",
    "        elif adj != word:\n",
    "            lematized_words.append(adj)\n",
    "        elif none != word:\n",
    "            lematized_words.append(none)\n",
    "        elif adv != word:\n",
    "            lematized_words.append(adv)\n",
    "        else:\n",
    "            lematized_words.append(word)\n",
    "            \n",
    "    return ' '.join(lematized_words)\n",
    "    #return ' '.join([lemmatizer.lemmatize(word, 'v') for word in NEC])\n",
    "    \n",
    "# Define function to slice text\n",
    "def slice_text(text, max_words=25):\n",
    "    words = text.split()\n",
    "    sliced_words = words[:max_words]\n",
    "    return ' '.join(sliced_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>265723</td>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>284269</td>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>207715</td>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>551106</td>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8584</td>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  label\n",
       "0  265723  A group of friends began to volunteer at a hom...      0\n",
       "1  284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
       "2  207715  In 1961, Goodyear released a kit that allows P...      0\n",
       "3  551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
       "4    8584  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('xy_train.csv') # reading the dataframe.\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function below do some preprocessing and clean the data\n",
    "def data_preprocessing(data):\n",
    "    data['text'] = data['text'].apply(slice_text) # get first 50 words of the text\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-z ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-z]\\b\", re.IGNORECASE)\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-z,.!? ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-z,.!?]\\b\", re.IGNORECASE)\n",
    "    data['text_'] = data['text'].apply(lambda x: re.sub(RE_TAGS,\" \" ,x)) ##Removing spac\n",
    "    data['text_'] = data['text'].apply(lambda x: re.sub(RE_ASCII, \" \", x)) ##Removing spac\n",
    "    data['text_'] = data['text'].apply(lambda x: re.sub(RE_SINGLECHAR, \" \", x)) ##Removing spac\n",
    "    data['text_'] = data['text'].apply(lambda x: re.sub(RE_WSPACE, \" \", x)) ##Removing spac\n",
    "    data['text_'] = data['text'].apply(lambda x: re.sub(r\"\\\\|\\n|\\\\\\n|\\r|\\t\",\" \" ,x)) ##Removing spaces\n",
    "    data['text_'] = data.text_.apply(lambda x: cont_to_exp(x,contractions)) # expand contractions such has I'm to I am \n",
    "    # test_['text_'] = test_.text_.apply(lambda x: cont_to_exp(x,contractions)\n",
    "    data['text_'] = data.text_.apply(lambda x: Prepro_1(x)) # replace links and mentions by MENTION or LINK words\n",
    "    data['text_'] = data.text_.apply(lambda x: cont_to_exp(x,numer_words)) #sperate numbers form words\n",
    "    data['text_'] = data.text_.apply(lambda x: cont_to_exp(x,word_numbers))\n",
    "    data['text_'] = data.text_.apply(lambda x: cont_to_exp(x,repetead_vowels))\n",
    "    data['text_'] = data.text_.apply(lambda x: re.sub(r\"\\b\\d+\\b\", \" number \",x))\n",
    "    data['text_'] = data.text_.apply(lambda x: re.sub(r\"\\b([0-9+]+[rd|th|nd]+)\\b\", \" number \",x)) # replace numbers by word `number`\n",
    "    data['text_'] = data.text_.apply(lambda x: re.sub(r\"([0-9]+[am|pm|hr|s|yr|yrs|hours|day|days|years|yesterday|year])\", \" time \",x)) # replace time by word `time`\n",
    "    data['text_'] = data.text_.apply(lambda x: re.sub(r\"([0-9]+[fps|mm|km|w|ft|oz|lbs|whts|kg]+)\",\" measure \",x)) # replace any measure by word `measure`\n",
    "    data['text_'] = data.text_.apply(lematization) # do lemmatization\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_preprocessing(train_data) # apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>265723</td>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "      <td>group friend begin volunteer homeless shelter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>284269</td>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "      <td>british prime minister mention nerve attack fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>207715</td>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "      <td>number goodyear release kit allow p time bring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>551106</td>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "      <td>happy birthday bob barker price right host hed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8584</td>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>obama nation 聙innocent cop unarm young black m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  label  \\\n",
       "0  265723  A group of friends began to volunteer at a hom...      0   \n",
       "1  284269  British Prime Minister @Theresa_May on Nerve A...      0   \n",
       "2  207715  In 1961, Goodyear released a kit that allows P...      0   \n",
       "3  551106  Happy Birthday, Bob Barker! The Price Is Right...      0   \n",
       "4    8584  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0   \n",
       "\n",
       "                                               text_  \n",
       "0  group friend begin volunteer homeless shelter ...  \n",
       "1  british prime minister mention nerve attack fo...  \n",
       "2  number goodyear release kit allow p time bring...  \n",
       "3  happy birthday bob barker price right host hed...  \n",
       "4  obama nation 聙innocent cop unarm young black m...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].unique() # print labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have 3 labels so we gonna convert label 2 to label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['label'] = train_data['label'].apply(lambda x: 1 if x == 2 else  x) # convert label 2 to label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].unique() # check if label changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year        3761\n",
       "get         3431\n",
       "make        3262\n",
       "one         3001\n",
       "time        2924\n",
       "find        2880\n",
       "new         2854\n",
       "say         2795\n",
       "like        2729\n",
       "look        2663\n",
       "man         2551\n",
       "take        2433\n",
       "trump       2355\n",
       "u           2308\n",
       "colorize    2290\n",
       "old         2255\n",
       "first       2129\n",
       "use         2101\n",
       "people      2056\n",
       "go          2045\n",
       "poster      1904\n",
       "woman       1737\n",
       "day         1732\n",
       "war         1700\n",
       "see         1619\n",
       "give        1523\n",
       "know        1466\n",
       "post        1443\n",
       "show        1442\n",
       "leave       1420\n",
       "american    1418\n",
       "work        1410\n",
       "life        1357\n",
       "world       1355\n",
       "think       1330\n",
       "come        1294\n",
       "鈥           1290\n",
       "state       1259\n",
       "school      1257\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.models import NumeralTickFormatter\n",
    "# Word Frequency of most common words\n",
    "word_freq = pd.Series(\" \".join(train_data[\"text_\"]).split()).value_counts() # count words after preprocessing\n",
    "word_freq[1:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>cartwright</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ground鈥</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>鈥楨lephant鈥檚</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>anoddsort</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2000脳1333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>btech</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>scorese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>鈥渂right</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>kal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>蜆蜄虡太o汀虩蛧蛶蛧虨贪f痰贪太蛵虪虧虡虡</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index  freq\n",
       "0             cartwright     1\n",
       "1                ground鈥     1\n",
       "2            鈥楨lephant鈥檚     1\n",
       "3              anoddsort     1\n",
       "4              2000脳1333     1\n",
       "5                  btech     1\n",
       "6                scorese     1\n",
       "7                鈥渂right     1\n",
       "8                    kal     1\n",
       "9  蜆蜄虡太o汀虩蛧蛶蛧虨贪f痰贪太蛵虪虧虡虡     1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list most uncommon words\n",
    "word_freq[-10:].reset_index(name=\"freq\") # lowest frequented words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43438"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_freq) # length of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5362\n",
       "1    0.4638\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of ratings\n",
    "train_data[\"label\"].value_counts(normalize=True) # check of the data balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "# Sample data - 20% of data to test set\n",
    "train, test = train_test_split(train_data, test_size=0.20, shuffle=True)\n",
    "\n",
    "X_train = train[\"text_\"]\n",
    "Y_train = train[\"label\"]\n",
    "X_test = test[\"text_\"]\n",
    "Y_test = test[\"label\"]\n",
    "print(X_train.shape) # print shape of train\n",
    "print(X_test.shape) # print shape of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two lines of code below define a vaildation set to use in grid search instead of cross vaildation.\n",
    "split_index = [-1 if x in X_train.index else 0 for x in train_data.index]\n",
    "pds = PredefinedSplit(test_fold = split_index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 using word analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=word, rf__n_estimators=100, score=0.854, total= 1.7min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.9, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.9, tfidf__analyzer=word, rf__n_estimators=100, score=0.854, total= 1.9min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.9, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.9, tfidf__analyzer=word, rf__n_estimators=100, score=0.858, total= 3.4min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word, rf__n_estimators=100, score=0.855, total=16.2min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 23.3min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.95, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.95, tfidf__analyzer=word, rf__n_estimators=100, score=0.852, total=15.2min\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 38.5min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.9, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.9, tfidf__analyzer=word, rf__n_estimators=100, score=0.853, total= 1.7min\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 40.3min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.9, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.9, tfidf__analyzer=word, rf__n_estimators=100, score=0.852, total=15.9min\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 56.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word, rf__n_estimators=100, score=0.853, total= 2.3min\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 58.5min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, rf__n_estimators=100, score=0.856, total= 4.0min\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 62.5min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.95, tfidf__analyzer=word, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.95, tfidf__analyzer=word, rf__n_estimators=100, score=0.853, total= 1.7min\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 64.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 64.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([ 0,  0, ..., -1, -1])),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('tfidf',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           1)...\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'rf__n_estimators': [100],\n",
       "                                        'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__max_df': [0.8, 0.9, 0.95],\n",
       "                                        'tfidf__min_df': [1, 2, 5, 10],\n",
       "                                        'tfidf__ngram_range': [(1, 2)],\n",
       "                                        'tfidf__norm': ['l2'],\n",
       "                                        'tfidf__stop_words': ['english']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_rf = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"rf\", RandomForestClassifier())])\n",
    "\n",
    "# define parameter space to test \n",
    "rf_params = {\n",
    "    'tfidf__analyzer':[\"word\"], # 1\n",
    "    'tfidf__max_df':[0.8, 0.9, 0.95], # 4\n",
    "    'tfidf__min_df':[1, 2, 5, 10], # 4\n",
    "    #'tfidf__max_features':[4000, 5000, 6000], # 3\n",
    "    'tfidf__norm':[\"l2\"], # 1\n",
    "     'tfidf__ngram_range': [(1, 2)], # 2\n",
    "     'tfidf__stop_words': ['english'], # 1\n",
    "     'rf__n_estimators':[100], # 2\n",
    "     #'rf__max_depth': [1000, 2000] # 2\n",
    "}\n",
    "\n",
    "\n",
    "pipe_rf = RandomizedSearchCV(pipe_rf, rf_params, verbose=100, cv=pds, scoring=\"roc_auc\", refit=True, n_iter=10)\n",
    "pipe_rf.fit(train_data['text_'], train_data['label']) # fit the random forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8579876728558414"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rf.best_score_ # print best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best score of random forest is 85.7% this is the first trial using those hyperparameters which we assumed they are good for this problem so we gonna compare this result with different hyperparamters and different models in the upcomming trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__stop_words': 'english',\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__min_df': 2,\n",
       " 'tfidf__max_df': 0.9,\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'rf__n_estimators': 100}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rf.best_params_ # print best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Reading and predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_orig = pd.read_csv('x_test.csv') # reading the test data\n",
    "test_data = data_preprocessing(test_data_orig.copy()) # do the preprocessing\n",
    "\n",
    "submission = pd.DataFrame() # create a dataframe\n",
    "submission['id'] = test_data_orig['id'] # get the id column\n",
    "\n",
    "submission['label'] = pipe_rf.predict_proba(test_data['text_'])[:,1] # predict the values\n",
    "\n",
    "submission.to_csv('sample_submission_walkthrough.csv', index=False) # save the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 using character analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 15 candidates, totalling 15 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100, score=0.750, total= 4.2min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100, score=0.751, total= 2.8min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=10, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=10, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100, score=0.802, total= 5.1min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 12.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100, score=0.825, total= 5.4min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 17.5min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=1, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=1, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100, score=0.824, total= 5.2min\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 22.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100, score=0.758, total= 3.9min\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 26.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=5, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=5, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100, score=0.825, total= 5.6min\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 32.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100, score=0.826, total= 5.6min\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 37.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=2, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=2, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100, score=0.826, total= 5.5min\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 43.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100, score=0.803, total= 5.5min\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 48.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100, score=0.801, total= 5.3min\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed: 54.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100, score=0.758, total= 3.1min\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 57.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100, score=0.800, total= 4.0min\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed: 61.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 4), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=char, rf__n_estimators=100, score=0.827, total= 5.2min\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed: 66.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100 \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.9, tfidf__analyzer=char, rf__n_estimators=100, score=0.802, total= 4.4min\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 70.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 70.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([ 0,  0, ..., -1, -1])),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('tfidf',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           1)...\n",
       "                   iid='warn', n_iter=15, n_jobs=None,\n",
       "                   param_distributions={'rf__n_estimators': [100],\n",
       "                                        'tfidf__analyzer': ['char'],\n",
       "                                        'tfidf__max_df': [0.8, 0.9],\n",
       "                                        'tfidf__min_df': [1, 2, 5, 10],\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
       "                                                               (1, 4)],\n",
       "                                        'tfidf__norm': ['l2'],\n",
       "                                        'tfidf__stop_words': ['english']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=100)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_rf = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"rf\", RandomForestClassifier())])\n",
    "\n",
    "# define parameter space to test \n",
    "rf_params = {\n",
    "    'tfidf__analyzer':[\"char\"], # 1\n",
    "    'tfidf__max_df':[0.8, 0.9], # 2\n",
    "    'tfidf__min_df':[1, 2, 5, 10], # 4\n",
    "    #'tfidf__max_features':[4000, 5000, 6000], # 3\n",
    "    'tfidf__norm':[\"l2\"], # 1\n",
    "     'tfidf__ngram_range': [(1, 2), (1, 3), (1, 4)], # 3\n",
    "     'tfidf__stop_words': ['english'], # 1\n",
    "     'rf__n_estimators':[100], # 2\n",
    "     #'rf__max_depth': [1000, 2000] # 2\n",
    "}\n",
    "\n",
    "\n",
    "pipe_rf_w = RandomizedSearchCV(pipe_rf, rf_params, verbose=100, cv=pds, scoring=\"roc_auc\", refit=True, n_iter=15)\n",
    "pipe_rf_w.fit(train_data['text_'], train_data['label']) # fit the random forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8266638598844015"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rf_w.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The trial above using char analyzer shows that it is worse than using word analyzer as it gets 82%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 12 is smaller than n_iter=25. Running 12 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 12 candidates, totalling 12 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word, score=0.819, total=  51.9s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   51.8s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word, score=0.831, total=  31.2s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.5, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.5, tfidf__analyzer=word, score=0.830, total=  22.7s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.5, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.5, tfidf__analyzer=word, score=0.830, total=  20.7s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word, score=0.819, total=  47.7s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.9min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, score=0.831, total=  29.6s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  3.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word, score=0.830, total=  22.9s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  3.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=word, score=0.830, total=  20.6s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  4.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.99, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.99, tfidf__analyzer=word, score=0.819, total=  47.3s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  4.9min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.99, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.99, tfidf__analyzer=word, score=0.831, total=  28.9s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  5.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word, score=0.830, total=  23.8s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  5.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.99, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.99, tfidf__analyzer=word, score=0.830, total=  21.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  6.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([ 0,  0, ..., -1, -1])),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('tfidf',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           1)...\n",
       "                                                           reg_alpha=None, ...))],\n",
       "                                      verbose=False),\n",
       "                   iid='warn', n_iter=25, n_jobs=None,\n",
       "                   param_distributions={'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__max_df': [0.5, 0.8, 0.99],\n",
       "                                        'tfidf__min_df': [1, 2, 5, 10],\n",
       "                                        'tfidf__ngram_range': [(1, 2)],\n",
       "                                        'tfidf__norm': ['l2'],\n",
       "                                        'tfidf__stop_words': ['english']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=100)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipe_ = Pipeline([('tfidf', TfidfVectorizer()), ('Model', xgb.XGBRegressor())]) # pipline for xgboost\n",
    "# xgboost parameters\n",
    "params = {\n",
    "        'tfidf__analyzer':[\"word\"], # 1\n",
    "        'tfidf__max_df':[0.5, 0.8, 0.99], # 3\n",
    "        'tfidf__min_df':[1, 2, 5, 10], # 4\n",
    "        #'tfidf__max_features':[2500 ,3000, 3500, 4000, 4500, 5000, 6000, 8000, 10000], # 5\n",
    "        'tfidf__norm':[\"l2\"], # 1\n",
    "        'tfidf__ngram_range': [(1, 2)], # 2\n",
    "        'tfidf__stop_words': ['english'], # 1\n",
    "        #'Model__min_child_weight': [1, 5, 10], # 3\n",
    "        #'Model__gamma': [0.5, 1, 1.5, 2, 5], # 5\n",
    "\n",
    "        #'Model__max_depth': [100,200,300,400,500,1000,2000] # 7\n",
    "        }\n",
    "xgb_gs = RandomizedSearchCV(Pipe_, params, cv = pds, verbose =100, scoring='roc_auc', refit=True, n_iter=25) # define random search\n",
    "xgb_gs.fit(train_data['text_'], train_data['label']) # fit random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8312627495370527"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gs.best_score_ # print best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best score of xgboost is 83% and it did worse than random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__stop_words': 'english',\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__min_df': 30,\n",
       " 'tfidf__max_df': 0.5,\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'Model__min_child_weight': 10,\n",
       " 'Model__gamma': 1}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gs.best_params_ # print best parameters of xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 16 is smaller than n_iter=60. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 16 candidates, totalling 16 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__analyzer=word, score=0.862, total=   6.1s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.0s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__analyzer=word, score=0.862, total=   9.2s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   15.4s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__analyzer=word, score=0.860, total=   4.7s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   20.3s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__analyzer=word, score=0.859, total=   9.1s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   29.5s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__analyzer=word, score=0.856, total=   4.9s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   34.6s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__analyzer=word, score=0.855, total=   8.6s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   43.3s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__analyzer=word, score=0.850, total=   4.9s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   48.4s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=10, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=10, tfidf__analyzer=word, score=0.849, total=   8.3s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   56.7s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=20, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=20, tfidf__analyzer=word, score=0.846, total=   4.2s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=20, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=20, tfidf__analyzer=word, score=0.845, total=   8.5s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=30, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=30, tfidf__analyzer=word, score=0.842, total=   4.3s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=30, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=30, tfidf__analyzer=word, score=0.841, total=   7.5s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=40, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=40, tfidf__analyzer=word, score=0.837, total=   4.5s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=40, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=40, tfidf__analyzer=word, score=0.836, total=   7.5s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=100, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=100, tfidf__analyzer=word, score=0.815, total=   5.0s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=100, tfidf__analyzer=word \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=100, tfidf__analyzer=word, score=0.814, total=   6.9s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([ 0,  0, ..., -1, -1])),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('tfidf',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           1)...\n",
       "                                                            fit_prior=True))],\n",
       "                                      verbose=False),\n",
       "                   iid='warn', n_iter=60, n_jobs=None,\n",
       "                   param_distributions={'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__min_df': [1, 2, 5, 10, 20, 30,\n",
       "                                                          40, 100],\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3)],\n",
       "                                        'tfidf__norm': ['l2'],\n",
       "                                        'tfidf__stop_words': ['english']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=100)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_pipe = Pipeline([('tfidf', TfidfVectorizer()), ('mb', MultinomialNB())]) # pipline for naive bayes\n",
    "\n",
    "# naive bayes paramters\n",
    "naive_params = {\n",
    "        'tfidf__analyzer':[\"word\"], # 1\n",
    "        #'tfidf__max_df':[0.5, 0.8, 0.95, 0.99], # 4\n",
    "        'tfidf__min_df':[1, 2, 5, 10, 20, 30, 40, 100], # 6\n",
    "        #'tfidf__max_features':[5000, 6000, 7000, 8000, 9000, 10000], # 5\n",
    "        'tfidf__norm':[\"l2\"], # 1\n",
    "        'tfidf__ngram_range': [(1, 2), (1, 3)], # 2\n",
    "        'tfidf__stop_words': ['english'] # 1\n",
    " \n",
    "}\n",
    "\n",
    "\n",
    "naive_gs = RandomizedSearchCV(naive_pipe, naive_params, cv = pds, verbose =100, scoring='roc_auc', refit=True, n_iter=60) # define random search for naive bayes\n",
    "naive_gs.fit(train_data['text_'], train_data['label']) # fit the random search for naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8620148626326889"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_gs.best_score_ # print best score for naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive bayes best score is 86.2% and it is much faster than Xgboost and Random forest as Random forest and XGBoost are ensemble methods that build multiple decision trees and combine their predictions. These algorithms are typically more computationally intensive than MultinomialNB because they involve building and evaluating multiple decision trees. Naive bayes did better than Xgboost and Random forest. so I think it is the best algorithm for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__stop_words': 'english',\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__analyzer': 'word'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_gs.best_params_ # print best parameters for naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I Think I should try an algorithm that take weights that vectorizer gives for each word in its consideration so I will try logistic regression I think it will do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs, score=0.863, total=  37.4s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   37.3s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=sag, score=0.871, total=  11.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.4s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=newton-cg, score=0.871, total=   8.6s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   57.1s remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=lbfgs, score=0.874, total=  14.2s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=newton-cg, score=0.874, total=   9.6s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs, score=0.867, total=   7.2s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=saga, score=0.874, total=   7.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs, score=0.868, total=   9.5s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  1.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=newton-cg, score=0.874, total=   9.4s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  1.9min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=sag, score=0.874, total=   6.9s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  2.0min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=lbfgs, score=0.863, total=  34.1s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  2.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=saga, score=0.874, total=   7.3s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  2.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=saga, score=0.874, total=   7.3s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  2.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=saga, score=0.874, total=   9.1s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:  3.0min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs, score=0.874, total=   9.4s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  3.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=saga, score=0.858, total=  14.9s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:  3.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=sag, score=0.871, total=   7.6s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:  3.5min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs, score=0.871, total=   7.9s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  3.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=saga, score=0.867, total=   7.5s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:  3.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=newton-cg, score=0.868, total=   8.6s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  3.9min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=saga, score=0.863, total=   9.3s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:  4.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=lbfgs, score=0.874, total=  22.7s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:  4.5min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=newton-cg, score=0.874, total=  16.7s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:  4.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=saga, score=0.870, total=  11.7s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  5.0min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=newton-cg, score=0.874, total=   9.9s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  5.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=sag, score=0.863, total=  18.1s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  5.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=sag, score=0.870, total=   9.4s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  5.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=newton-cg, score=0.871, total=   7.5s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:  5.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=newton-cg, score=0.868, total=  12.3s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:  5.9min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=newton-cg, score=0.858, total=  22.7s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  6.3min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=newton-cg, score=0.870, total=  17.2s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:  6.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=sag, score=0.874, total=   8.7s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:  6.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=10, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=10, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs, score=0.864, total=  13.6s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:  7.0min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=saga, score=0.867, total=  13.0s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:  7.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=lbfgs, score=0.863, total=  40.4s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:  7.9min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=sag, score=0.868, total=  13.1s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  8.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=sag, score=0.874, total=   7.4s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:  8.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=saga, score=0.874, total=   9.8s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:  8.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=newton-cg, score=0.874, total=  17.2s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:  8.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs, score=0.874, total=  16.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  8.9min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=5, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=lbfgs, score=0.874, total=  16.6s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:  9.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs, score=0.863, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:  9.5min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=saga, score=0.874, total=  14.4s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:  9.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=sag, score=0.858, total=  16.0s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed: 10.0min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=newton-cg, score=0.870, total=  14.2s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed: 10.3min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=saga, score=0.874, total=  12.6s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed: 10.5min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=saga, score=0.871, total=   9.4s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed: 10.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=lbfgs, score=0.874, total=  10.2s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 10.8min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=saga \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=saga, score=0.874, total=  12.7s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed: 11.0min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=2, tfidf__max_df=0.8, tfidf__analyzer=word, log__solver=newton-cg, score=0.874, total=  11.0s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 11.2min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs, score=0.863, total=  20.5s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed: 11.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=sag, score=0.863, total=   8.1s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed: 11.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=lbfgs, score=0.870, total=  21.8s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed: 12.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=10, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=lbfgs, score=0.871, total=  18.0s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed: 12.4min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=sag, score=0.867, total=  16.2s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed: 12.6min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=5, tfidf__max_df=0.1, tfidf__analyzer=word, log__solver=lbfgs, score=0.867, total=  14.7s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed: 12.9min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=sag \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.5, tfidf__analyzer=word, log__solver=sag, score=0.874, total=  15.3s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed: 13.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=1, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=lbfgs, score=0.863, total=  36.2s\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed: 13.7min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=lbfgs \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 2), tfidf__min_df=1, tfidf__max_df=0.99, tfidf__analyzer=word, log__solver=lbfgs, score=0.870, total=  22.3s\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed: 14.1min remaining:    0.0s\n",
      "[CV] tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=newton-cg \n",
      "[CV]  tfidf__stop_words=english, tfidf__norm=l2, tfidf__ngram_range=(1, 3), tfidf__min_df=2, tfidf__max_df=0.95, tfidf__analyzer=word, log__solver=newton-cg, score=0.874, total=  12.6s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 14.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 14.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([ 0,  0, ..., -1, -1])),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('tfidf',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           1)...\n",
       "                   param_distributions={'log__solver': ['lbfgs', 'sag', 'saga',\n",
       "                                                        'newton-cg'],\n",
       "                                        'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__max_df': [0.1, 0.5, 0.8, 0.95,\n",
       "                                                          0.99],\n",
       "                                        'tfidf__min_df': [1, 2, 5, 10],\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3)],\n",
       "                                        'tfidf__norm': ['l2'],\n",
       "                                        'tfidf__stop_words': ['english']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=100)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_pip = Pipeline([('tfidf', TfidfVectorizer()), ('log', LogisticRegression())]) # pipline for naive bayes\n",
    "\n",
    "# naive bayes paramters\n",
    "naive_params = {\n",
    "        'tfidf__analyzer':[\"word\"], # 1\n",
    "        'tfidf__max_df':[0.1, 0.5, 0.8, 0.95, 0.99], # 4\n",
    "        'tfidf__min_df':[1, 2, 5, 10], # 4\n",
    "        #'tfidf__max_features':[5000, 6000, 7000, 8000, 9000, 10000], # 5\n",
    "        'tfidf__norm':[\"l2\"], # 1\n",
    "        'tfidf__ngram_range': [(1, 2), (1, 3)], # 2\n",
    "        'tfidf__stop_words': ['english'], # 1\n",
    "        'log__solver':['lbfgs', 'sag', 'saga', 'newton-cg']\n",
    " \n",
    "}\n",
    "\n",
    "\n",
    "log_gs = RandomizedSearchCV(log_pip, naive_params, cv = pds, verbose =100, scoring='roc_auc', refit=True, n_iter=60) # define random search for naive bayes\n",
    "log_gs.fit(train_data['text_'], train_data['label']) # fit the random search for naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8744059107103052"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic regression gets an 87.4% score. the reason I think behind this because it takes the weights of number of each word in its consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__stop_words': 'english',\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__min_df': 2,\n",
       " 'tfidf__max_df': 0.5,\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'log__solver': 'sag'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_orig = pd.read_csv('x_test.csv') # reading the test data\n",
    "test_data = data_preprocessing(test_data_orig.copy()) # do the preprocessing\n",
    "\n",
    "submission = pd.DataFrame() # create a dataframe\n",
    "submission['id'] = test_data_orig['id'] # get the id column\n",
    "\n",
    "submission['label'] = log_gs.predict_proba(test_data['text_'])[:,1] # predict the values\n",
    "\n",
    "submission.to_csv('sample_submission_walkthrough.csv', index=False) # save the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conculsion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessing done using lemmatization\n",
    "- we used different approch in vectorizer which is word and character analyzer and we use different n-grams\n",
    "- (1, 2) word analyzer is much better than (1, 2), (1, 3), (1, 4) character analyzer for this problem\n",
    "- we used different models like random forest, xgboost and naive bayes\n",
    "- Naive bayes best score is 86.2% and it is much faster than Xgboost and Random forest as Random forest and XGBoost as Random forest and XGBoost are ensemble methods\n",
    "- logistic regression gets an 87.4% score. the reason I think behind this because it takes the weights of number of each word in its consideration and it's faster than Xgboost and Random forest as Random forest so I think it is the suitable one among other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. References:\n",
    "- https://enjoymachinelearning.com/blog/countvectorizer-vs-tfidfvectorizer/\n",
    "- https://www.machinelearningplus.com/nlp/what-is-tokenization-in-natural-language-processing/\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
